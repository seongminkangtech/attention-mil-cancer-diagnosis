#!/usr/bin/env python3
"""
PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸

Attention MIL ëª¨ë¸ì„ ONNX í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í”„ë¡œë•ì…˜ ë°°í¬ì— ìµœì í™”í•©ë‹ˆë‹¤.
"""

import argparse
import os
import sys
import torch
import onnx
import onnxruntime
import numpy as np
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from src.models.attention_mil import AttentionMILModel
from src.models.feature_extractor import FeatureExtractor
from src.utils.config import load_config


def create_dummy_input(batch_size: int = 1, num_tiles: int = 50, channels: int = 3, height: int = 256, width: int = 256):
    """
    ONNX ë³€í™˜ì„ ìœ„í•œ ë”ë¯¸ ì…ë ¥ ìƒì„±
    
    Args:
        batch_size (int): ë°°ì¹˜ í¬ê¸°
        num_tiles (int): íƒ€ì¼(í”„ë ˆì„) ìˆ˜
        channels (int): ì±„ë„ ìˆ˜
        height (int): ì´ë¯¸ì§€ ë†’ì´
        width (int): ì´ë¯¸ì§€ ë„ˆë¹„
        
    Returns:
        torch.Tensor: ë”ë¯¸ ì…ë ¥ í…ì„œ
    """
    return torch.randn(batch_size, num_tiles, channels, height, width)


def convert_to_onnx(model: torch.nn.Module, 
                   dummy_input: torch.Tensor, 
                   output_path: str,
                   input_names: list = None,
                   output_names: list = None,
                   dynamic_axes: dict = None,
                   opset_version: int = 11):
    """
    PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜
    
    Args:
        model (torch.nn.Module): ë³€í™˜í•  PyTorch ëª¨ë¸
        dummy_input (torch.Tensor): ë”ë¯¸ ì…ë ¥ í…ì„œ
        output_path (str): ì¶œë ¥ ONNX íŒŒì¼ ê²½ë¡œ
        input_names (list): ì…ë ¥ í…ì„œ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        output_names (list): ì¶œë ¥ í…ì„œ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        dynamic_axes (dict): ë™ì  ì¶• ì„¤ì •
        opset_version (int): ONNX opset ë²„ì „
    """
    print(f"ğŸ”„ PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜ ì¤‘...")
    print(f"   - ì…ë ¥ í…ì„œ í¬ê¸°: {dummy_input.shape}")
    print(f"   - ì¶œë ¥ ê²½ë¡œ: {output_path}")
    print(f"   - ONNX opset ë²„ì „: {opset_version}")
    
    # ê¸°ë³¸ ì…ë ¥/ì¶œë ¥ ì´ë¦„ ì„¤ì •
    if input_names is None:
        input_names = ['input']
    if output_names is None:
        output_names = ['output']
    
    # ê¸°ë³¸ ë™ì  ì¶• ì„¤ì •
    if dynamic_axes is None:
        dynamic_axes = {
            'input': {0: 'batch_size', 1: 'num_tiles'},
            'output': {0: 'batch_size'}
        }
    
    # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    model.eval()
    
    try:
        # ONNX ë³€í™˜
        torch.onnx.export(
            model,
            dummy_input,
            output_path,
            export_params=True,
            opset_version=opset_version,
            do_constant_folding=True,
            input_names=input_names,
            output_names=output_names,
            dynamic_axes=dynamic_axes,
            verbose=False
        )
        
        print(f"âœ… ONNX ë³€í™˜ ì„±ê³µ!")
        print(f"   - íŒŒì¼ í¬ê¸°: {os.path.getsize(output_path) / (1024*1024):.2f} MB")
        
    except Exception as e:
        print(f"âŒ ONNX ë³€í™˜ ì‹¤íŒ¨: {e}")
        raise


def validate_onnx_model(onnx_path: str, dummy_input: torch.Tensor):
    """
    ë³€í™˜ëœ ONNX ëª¨ë¸ ê²€ì¦
    
    Args:
        onnx_path (str): ONNX ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        dummy_input (torch.Tensor): ê²€ì¦ìš© ë”ë¯¸ ì…ë ¥
    """
    print(f"ğŸ” ONNX ëª¨ë¸ ê²€ì¦ ì¤‘...")
    
    try:
        # ONNX ëª¨ë¸ ë¡œë“œ ë° ê²€ì¦
        onnx_model = onnx.load(onnx_path)
        onnx.checker.check_model(onnx_model)
        print(f"âœ… ONNX ëª¨ë¸ í˜•ì‹ ê²€ì¦ ì„±ê³µ")
        
        # ONNX Runtimeìœ¼ë¡œ ì¶”ë¡  í…ŒìŠ¤íŠ¸
        ort_session = onnxruntime.InferenceSession(onnx_path)
        
        # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
        input_data = dummy_input.numpy()
        input_name = ort_session.get_inputs()[0].name
        
        # ì¶”ë¡  ì‹¤í–‰
        output = ort_session.run(None, {input_name: input_data})
        
        print(f"âœ… ONNX Runtime ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì„±ê³µ")
        print(f"   - ì¶œë ¥ í˜•íƒœ: {output[0].shape}")
        print(f"   - ì¶œë ¥ ë°ì´í„° íƒ€ì…: {output[0].dtype}")
        
    except Exception as e:
        print(f"âŒ ONNX ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
        raise


def load_pytorch_model(config_path: str, model_path: str):
    """
    PyTorch ëª¨ë¸ ë¡œë“œ
    
    Args:
        config_path (str): ì„¤ì • íŒŒì¼ ê²½ë¡œ
        model_path (str): ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œ
        
    Returns:
        torch.nn.Module: ë¡œë“œëœ ëª¨ë¸
    """
    print(f"ğŸ“¥ PyTorch ëª¨ë¸ ë¡œë“œ ì¤‘...")
    print(f"   - ì„¤ì • íŒŒì¼: {config_path}")
    print(f"   - ëª¨ë¸ ê°€ì¤‘ì¹˜: {model_path}")
    
    # ì„¤ì • ë¡œë“œ
    config = load_config(config_path)
    
    # íŠ¹ì§• ì¶”ì¶œê¸° ìƒì„±
    feature_extractor = FeatureExtractor(
        model_name=config.get('feature_extractor', {}).get('model_name', 'efficientnet_b2'),
        pretrained=config.get('feature_extractor', {}).get('pretrained', False)
    )
    
    # Attention MIL ëª¨ë¸ ìƒì„±
    model = AttentionMILModel(
        num_classes=config.get('model', {}).get('num_classes', 3),
        feature_extractor=feature_extractor,
        attention_hidden_dim=config.get('model', {}).get('attention_hidden_dim', 128),
        dropout_rate=config.get('model', {}).get('dropout_rate', 0.2)
    )
    
    # ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ
    if os.path.exists(model_path):
        checkpoint = torch.load(model_path, map_location='cpu')
        if 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
        else:
            model.load_state_dict(checkpoint)
        print(f"âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì„±ê³µ")
    else:
        print(f"âš ï¸ ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì´ˆê¸°í™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    return model


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜')
    parser.add_argument('--config', type=str, default='configs/model_configs/attention_mil.yaml',
                       help='ëª¨ë¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--model_path', type=str, required=True,
                       help='PyTorch ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ ê²½ë¡œ (.pth)')
    parser.add_argument('--output_path', type=str, default='models/attention_mil.onnx',
                       help='ì¶œë ¥ ONNX íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--batch_size', type=int, default=1,
                       help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--num_tiles', type=int, default=50,
                       help='íƒ€ì¼(í”„ë ˆì„) ìˆ˜')
    parser.add_argument('--height', type=int, default=256,
                       help='ì´ë¯¸ì§€ ë†’ì´')
    parser.add_argument('--width', type=int, default=256,
                       help='ì´ë¯¸ì§€ ë„ˆë¹„')
    parser.add_argument('--opset_version', type=int, default=11,
                       help='ONNX opset ë²„ì „')
    parser.add_argument('--validate', action='store_true',
                       help='ë³€í™˜ í›„ ONNX ëª¨ë¸ ê²€ì¦ ìˆ˜í–‰')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ PyTorch â†’ ONNX ë³€í™˜ ì‹œì‘")
    print(f"   - ì„¤ì • íŒŒì¼: {args.config}")
    print(f"   - ëª¨ë¸ ê²½ë¡œ: {args.model_path}")
    print(f"   - ì¶œë ¥ ê²½ë¡œ: {args.output_path}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(args.output_path), exist_ok=True)
    
    try:
        # PyTorch ëª¨ë¸ ë¡œë“œ
        model = load_pytorch_model(args.config, args.model_path)
        
        # ë”ë¯¸ ì…ë ¥ ìƒì„±
        dummy_input = create_dummy_input(
            batch_size=args.batch_size,
            num_tiles=args.num_tiles,
            height=args.height,
            width=args.width
        )
        
        # ONNX ë³€í™˜
        convert_to_onnx(
            model=model,
            dummy_input=dummy_input,
            output_path=args.output_path,
            opset_version=args.opset_version
        )
        
        # ê²€ì¦ ìˆ˜í–‰ (ì„ íƒì‚¬í•­)
        if args.validate:
            validate_onnx_model(args.output_path, dummy_input)
        
        print(f"ğŸ‰ ONNX ë³€í™˜ ì™„ë£Œ!")
        print(f"   - ì¶œë ¥ íŒŒì¼: {args.output_path}")
        
    except Exception as e:
        print(f"âŒ ë³€í™˜ ì‹¤íŒ¨: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 